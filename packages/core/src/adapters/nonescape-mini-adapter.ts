import type { MlModelRunner } from '../types.js';
import { registerMlModel } from '../detectors/image-detector.js';

export interface NonescapeModelApi {
  predict(input: {
    data: Uint8ClampedArray;
    width: number;
    height: number;
    features: NonescapeModelFeatures;
  }): number | Promise<number>;
}

export interface NonescapeModelFeatures {
  meanSat: number;
  satVar: number;
  meanLum: number;
  channelVarSimilarity: number;
  /** Mean luminance gradient magnitude (per-pixel rate, 0–1). Lower = smoother = more AI-like. */
  gradientMean: number;
  /** Population variance of luminance values (0–~0.25). */
  lumVariance: number;
  /**
   * Mean luminance variance in the flattest 20% of 8×8 blocks.
   * Diffusion models fully denoise, leaving near-zero variance in smooth regions.
   * Real JPEG photos retain JPEG block artifacts + sensor noise (higher floor).
   * Reference: Corvi et al., "On the Detection of Synthetic Images Generated by
   *   Diffusion Models" (ICASSP 2023).
   */
  noiseFloor: number;
  /**
   * Coefficient of variation (std/mean) of 8×8-block luminance variances.
   * Real photos: high CoV from depth-of-field and motion blur diversity.
   * AI images: spatially uniform quality → low CoV.
   * Reference: Simoncelli & Olshausen, "Natural image statistics and neural
   *   representation" (Annu. Rev. Neurosci. 2001).
   */
  textureCoV: number;
  /**
   * Fraction of stride-sampled interior pixels with near-zero discrete Laplacian
   * (5-tap cross: |4I₀ − I_L − I_R − I_U − I_D| ≤ threshold).
   * AI diffusion outputs are smooth → dense near-zero Laplacian → high sparsity.
   * Real photos carry sensor noise that fills in the residual distribution.
   * Reference: Fridrich & Kodovsky, "Rich Models for Steganalysis of Digital
   *   Images" (IEEE TIFS 2012); Matern et al., "Exploiting Visual Artifacts to
   *   Expose Deepfakes and Face Manipulations" (WACV 2019).
   */
  laplacianSparsity: number;
}

export interface NonescapeMiniAdapterOptions {
  /** Model profile key. Defaults to 'nonescape-mini'. */
  model?: string;
  /**
   * Optional model API implementation.
   * Pass this to swap in a new bundled model runtime without changing detector code.
   */
  api?: NonescapeModelApi;
}

const DEFAULT_MODEL_NAME = 'nonescape-mini';

const builtInModels: Record<string, NonescapeModelApi> = {
  'nonescape-mini': {
    predict({ features }): number {
      // ── Derived sub-scores from new research-backed features ──────────────────

      // Gradient smoothness (1st-order): AI diffusion outputs have smooth
      // per-pixel transitions (no sensor noise). Score 1 when gradient = 0,
      // 0 when per-pixel gradient ≥ 1/16.
      const gradSmoothnessScore = Math.max(0, 1 - features.gradientMean * 16);

      // Luminance variance: AI images tend to have moderate exposure range.
      // Very high lum variance (> 0.14) is more common in high-contrast real photos.
      const lumVarScore = Math.max(0, 1 - Math.max(0, features.lumVariance - 0.04) / 0.10);

      // Noise floor (Corvi et al. 2023): variance of flat blocks.
      // AI: near-zero (<0.0002); real JPEG: ≥0.0008 (JPEG quant noise + sensor).
      // Score: 1 when noiseFloor ≈ 0, 0 when noiseFloor ≥ 0.0010.
      const noiseFloorScore = Math.max(0, 1 - features.noiseFloor / 0.0010);

      // Texture CoV (Simoncelli & Olshausen 2001): spatial diversity of texture.
      // Real photos: depth-of-field gives CoV ≈ 2–6. AI: uniform CoV ≈ 0.3–1.5.
      // Score: 1 when CoV = 0, 0 when CoV ≥ 2.0.
      const textureUniformityScore = Math.max(0, 1 - features.textureCoV / 2.0);

      // Laplacian sparsity (Fridrich & Kodovsky 2012; Matern et al. 2019):
      // AI: most pixels smooth → sparsity ≈ 0.75–0.95.
      // Real noisy photos: sensor noise lifts residuals → sparsity ≈ 0.45–0.70.
      // Score: 0 when sparsity ≤ 0.65, 1 when sparsity ≥ 0.90.
      const lapSparsityScore = Math.max(0, Math.min(1, (features.laplacianSparsity - 0.65) / 0.25));

      // ── Logistic regression ──────────────────────────────────────────────────
      // Bias reduced from -1.80 to -2.80 to account for the three new positive
      // feature contributions while maintaining the desired operating point:
      //   typical AI portrait  → linear ≈ +3.2 → sigmoid ≈ 0.96 → score 0.95
      //   uncertain real photo → linear ≈ +0.2 → sigmoid ≈ 0.55 (uncertain, escalate)
      //   clear real photo     → linear ≈ -1.3 → sigmoid ≈ 0.21 → score 0.05
      const linear =
        -2.80 +
        features.meanSat * 2.6 +
        (1 - features.satVar * 8) * 1.2 +
        (1 - Math.abs(features.meanLum - 0.5) * 2) * 0.9 +
        features.channelVarSimilarity * 0.7 +
        gradSmoothnessScore * 0.8 +
        lumVarScore * 0.4 +
        noiseFloorScore * 1.0 +
        textureUniformityScore * 0.6 +
        lapSparsityScore * 0.7;
      const score = 1 / (1 + Math.exp(-linear));
      return Math.max(0, Math.min(1, score));
    },
  },
};

function classifyFromScore(score: number): number {
  // Calibrate away from hard binary outputs so uncertain samples can escalate
  // to remote ML in the cascade. Keep strong confidence near edges.
  if (score >= 0.9) return 0.95;
  if (score <= 0.1) return 0.05;
  return Math.max(0, Math.min(1, score));
}

/** BT.601 luminance in the 0–255 range from raw RGBA pixel offset `i`. */
function luma255(data: Uint8ClampedArray, i: number): number {
  return (data[i] * 299 + data[i + 1] * 587 + data[i + 2] * 114) / 1000;
}

function extractFeatures(data: Uint8ClampedArray, width: number, height: number): NonescapeModelFeatures {
  const pixelCount = data.length / 4;
  if (pixelCount === 0) {
    return {
      meanSat: 0, satVar: 0, meanLum: 0, channelVarSimilarity: 0,
      gradientMean: 0, lumVariance: 0, noiseFloor: 0, textureCoV: 0, laplacianSparsity: 0,
    };
  }

  let satSum = 0;
  let satSqSum = 0;
  let lumSum = 0;
  let lumSqSum = 0;
  let rSum = 0, gSum = 0, bSum = 0;
  let rSqSum = 0, gSqSum = 0, bSqSum = 0;

  for (let i = 0; i < data.length; i += 4) {
    const r8 = data[i], g8 = data[i + 1], b8 = data[i + 2];
    const r = r8 / 255, g = g8 / 255, b = b8 / 255;

    const max = Math.max(r, g, b);
    const sat = max === 0 ? 0 : (max - Math.min(r, g, b)) / max;
    satSum += sat;
    satSqSum += sat * sat;
    const lum = r * 0.299 + g * 0.587 + b * 0.114;
    lumSum += lum;
    lumSqSum += lum * lum;

    rSum += r8; gSum += g8; bSum += b8;
    rSqSum += r8 * r8; gSqSum += g8 * g8; bSqSum += b8 * b8;
  }

  const meanSat = satSum / pixelCount;
  const satVar = Math.max(0, satSqSum / pixelCount - meanSat * meanSat);
  const meanLum = lumSum / pixelCount;
  const lumVariance = Math.max(0, lumSqSum / pixelCount - meanLum * meanLum);

  const rMean = rSum / pixelCount;
  const gMean = gSum / pixelCount;
  const bMean = bSum / pixelCount;
  const rVar = Math.max(0, rSqSum / pixelCount - rMean * rMean);
  const gVar = Math.max(0, gSqSum / pixelCount - gMean * gMean);
  const bVar = Math.max(0, bSqSum / pixelCount - bMean * bMean);
  const varMean = (rVar + gVar + bVar) / 3;
  const channelVarSimilarity =
    varMean > 0
      ? Math.max(
          0,
          Math.min(
            1,
            1 -
              (Math.abs(rVar - varMean) + Math.abs(gVar - varMean) + Math.abs(bVar - varMean)) /
                (3 * varMean)
          )
        )
      : 1;

  // Gradient mean: mean per-pixel luminance gradient magnitude.
  // Stride-sampled to cap computation at ≈128×128 points regardless of image size,
  // then divided by stride to obtain a per-pixel rate comparable across resolutions.
  const gStride = Math.max(1, Math.ceil(Math.max(width, height) / 128));
  let gradSum = 0;
  let gradCount = 0;
  for (let y = 0; y < height - gStride; y += gStride) {
    for (let x = 0; x < width - gStride; x += gStride) {
      const i0 = (y * width + x) * 4;
      const i1 = (y * width + x + gStride) * 4;       // right neighbour
      const i2 = ((y + gStride) * width + x) * 4;    // bottom neighbour
      // Luminance in 0–255 range via BT.601 helper
      const l0 = luma255(data, i0);
      const l1 = luma255(data, i1);
      const l2 = luma255(data, i2);
      // Divide by: 2 (average two directions) × gStride (per-pixel rate) × 255 (→ [0,1])
      gradSum += (Math.abs(l1 - l0) + Math.abs(l2 - l0)) / (2 * gStride * 255);
      gradCount++;
    }
  }
  const gradientMean = gradCount > 0 ? gradSum / gradCount : 0;

  // ── Block-based texture analysis ──────────────────────────────────────────
  // Divide image into 8×8 blocks; stride-sample block origins to ≈64×64 grid.
  // Block luminance is normalised to [0,1] so variances are directly comparable.
  const BLK = 8;
  const blkStep = Math.max(BLK, Math.ceil(Math.max(width, height) / 64) * BLK);
  const blockVars: number[] = [];

  for (let by = 0; by + BLK <= height; by += blkStep) {
    for (let bx = 0; bx + BLK <= width; bx += blkStep) {
      let bLumSum = 0, bLumSqSum = 0;
      const bN = BLK * BLK;
      for (let dy = 0; dy < BLK; dy++) {
        for (let dx = 0; dx < BLK; dx++) {
          const i = ((by + dy) * width + (bx + dx)) * 4;
          const lv = luma255(data, i) / 255; // normalised to [0,1]
          bLumSum += lv;
          bLumSqSum += lv * lv;
        }
      }
      const bMean = bLumSum / bN;
      blockVars.push(Math.max(0, bLumSqSum / bN - bMean * bMean));
    }
  }

  // noiseFloor: mean variance of the smoothest 20% of blocks.
  // AI diffusion models remove all noise → near-zero flat-block variance.
  // Real JPEG photos retain quantisation noise → higher floor.
  let noiseFloor = 0;
  if (blockVars.length > 0) {
    const sorted = blockVars.slice().sort((a, b) => a - b);
    const nFlat = Math.max(1, Math.floor(sorted.length * 0.20));
    noiseFloor = sorted.slice(0, nFlat).reduce((a, b) => a + b, 0) / nFlat;
  }

  // textureCoV: coefficient of variation of block variances.
  // Low CoV = spatially uniform texture = more AI-like.
  // Guard: require meaningful mean block variance (> 1e-4 on 0–1 lum scale) so
  // that solid-colour inputs don't get CoV = 0 due to floating-point rounding.
  let textureCoV = 0;
  if (blockVars.length > 1) {
    const bvMean = blockVars.reduce((a, b) => a + b, 0) / blockVars.length;
    if (bvMean > 1e-4) {
      const bvVar = blockVars.reduce((a, b) => a + (b - bvMean) ** 2, 0) / blockVars.length;
      textureCoV = Math.sqrt(bvVar) / bvMean;
    }
  }

  // Laplacian sparsity: fraction of stride-sampled interior pixels whose
  // 5-tap discrete Laplacian magnitude is ≤ lapThreshold (0–255 scale).
  // AI diffusion outputs → smooth → high sparsity.
  // Real photos → sensor noise fills residual distribution → lower sparsity.
  // Threshold scales with stride so the per-pixel noise floor is consistent.
  const lapStride = Math.max(1, Math.ceil(Math.max(width, height) / 256));
  const lapThreshold = 5 * lapStride; // ≈ sensor noise floor scaled to stride
  let lapSparse = 0, lapCount = 0;
  for (let y = lapStride; y < height - lapStride; y += lapStride) {
    for (let x = lapStride; x < width - lapStride; x += lapStride) {
      const i0 = (y * width + x) * 4;
      const iL = (y * width + x - lapStride) * 4;
      const iR = (y * width + x + lapStride) * 4;
      const iU = ((y - lapStride) * width + x) * 4;
      const iD = ((y + lapStride) * width + x) * 4;
      // Luminance in 0–255 range via BT.601 helper
      const l0 = luma255(data, i0);
      const lL = luma255(data, iL);
      const lR = luma255(data, iR);
      const lU = luma255(data, iU);
      const lD = luma255(data, iD);
      // 5-tap cross Laplacian: L = 4·I₀ − I_L − I_R − I_U − I_D
      if (Math.abs(4 * l0 - lL - lR - lU - lD) <= lapThreshold) lapSparse++;
      lapCount++;
    }
  }
  const laplacianSparsity = lapCount > 0 ? lapSparse / lapCount : 0;

  return {
    meanSat, satVar, meanLum, channelVarSimilarity,
    gradientMean, lumVariance, noiseFloor, textureCoV, laplacianSparsity,
  };
}

function resolveModelApi(options: NonescapeMiniAdapterOptions): NonescapeModelApi {
  if (options.api) return options.api;
  const selectedModel = options.model ?? DEFAULT_MODEL_NAME;
  return builtInModels[selectedModel] ?? builtInModels[DEFAULT_MODEL_NAME];
}

export function createNonescapeMiniRunner(
  options: NonescapeMiniAdapterOptions = {}
): MlModelRunner {
  const modelApi = resolveModelApi(options);

  return {
    async run(data: Uint8ClampedArray, width: number, height: number): Promise<number> {
      const features = extractFeatures(data, width, height);
      const rawScore = await modelApi.predict({ data, width, height, features });
      return classifyFromScore(Math.max(0, Math.min(1, rawScore)));
    },
  };
}

export function registerNonescapeMiniModel(options: NonescapeMiniAdapterOptions = {}): void {
  registerMlModel(createNonescapeMiniRunner(options));
}
